{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3160bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests\n",
    "#pip install beautifulsoup4\n",
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c302ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3f9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = \"orgaos_interlegis(2025) - api_url.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "api_url= pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the URLs are in the first column (adjust column name if needed)\n",
    "api_urls = api_url.iloc[:, 0].dropna().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e22da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch data from API\n",
    "def fetch_data(api_url):\n",
    "    try:\n",
    "        response = requests.get(api_url)  # Send a GET request to the API URL\n",
    "        response.raise_for_status()  # Raise an exception for bad response status codes\n",
    "        return response.json()  # Return JSON data if the request is successful\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data from {api_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b0169ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data for each API URL\n",
    "def save_data(data, api_url, page_number):\n",
    "    if data is None:  # Check if data is None\n",
    "        print(f\"No data to save for {api_url}, page {page_number}.\")\n",
    "        return  # Return early if data is None\n",
    "    city_name = api_url.split(\".\")[1]  # Extract city name from the URL\n",
    "    city_file = f\"SAPL-Cidades_2025/{city_name}_data.csv\"  # File name for the city data\n",
    "    city_df = pd.DataFrame(data)  # Create a DataFrame from the fetched data\n",
    "    if os.path.exists(city_file):  # If the file already exists, append data to it\n",
    "        city_df.to_csv(city_file, mode='a', header=False, index=False)\n",
    "    else:  # If the file doesn't exist, create it and write data to it\n",
    "        city_df.to_csv(city_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd6956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to iterate through pages and fetch data\n",
    "def fetch_all_data(api_url, start_page=1):\n",
    "    all_data = []  # Initialize an empty list to store all data\n",
    "    next_url = api_url  # Initialize the next URL to the provided API URL\n",
    "    page_number = start_page  # Initialize the page number\n",
    "    while next_url:\n",
    "        print(f\"Processing page {page_number} of {api_url}...\")  # Print current page being processed\n",
    "        data = fetch_data(next_url)  # Fetch data from the current URL\n",
    "        if not data:  # If data retrieval fails, exit the loop\n",
    "            break\n",
    "        all_data.extend(data[\"results\"])  # Extend the list with data from the current page\n",
    "        next_url = data[\"pagination\"][\"links\"].get(\"next\")  # Get the URL for the next page, if available\n",
    "        if next_url:\n",
    "            time.sleep(0.2)  # Wait for X seconds before making the next request to respect API limits\n",
    "            page_number += 1  # Increment the page number\n",
    "            # Save data for each page\n",
    "            save_data(data[\"results\"], api_url, page_number)\n",
    "            # Update progress after processing each page\n",
    "            with open(progress_file, \"w\") as file:\n",
    "                file.write(f\"{i}\\n\")  # Write the last processed API URL\n",
    "                file.write(f\"{page_number}\\n\")  # Write the last processed page\n",
    "                file.write(next_url if next_url else \"\")  # Write the last processed next URL\n",
    "    return all_data  # Return the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "186b1729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://sapl.vilanovadosmartirios.ma.leg.br/api/materia/materialegislativa/...\n",
      "Processing page 10 of https://sapl.vilanovadosmartirios.ma.leg.br/api/materia/materialegislativa/?page=10...\n",
      "Processing https://sapl.pimenteiras.pi.leg.br/api/materia/materialegislativa/...\n",
      "Processing page 1 of https://sapl.pimenteiras.pi.leg.br/api/materia/materialegislativa/...\n",
      "Processing https://sapl.chapadadonorte.mg.leg.br/api/materia/materialegislativa/...\n",
      "Processing page 1 of https://sapl.chapadadonorte.mg.leg.br/api/materia/materialegislativa/...\n",
      "Processing https://sapl.borrazopolis.pr.leg.br/api/materia/materialegislativa/...\n",
      "Processing page 1 of https://sapl.borrazopolis.pr.leg.br/api/materia/materialegislativa/...\n",
      "Processing https://sapl.santaterezadotocantins.to.leg.br/api/materia/materialegislativa/...\n",
      "Processing page 1 of https://sapl.santaterezadotocantins.to.leg.br/api/materia/materialegislativa/...\n",
      "Processing https://sapl.carangola.mg.leg.br/api/materia/materialegislativa/...\n",
      "Processing page 1 of https://sapl.carangola.mg.leg.br/api/materia/materialegislativa/...\n",
      "Processing https://sapl.serradonavio.ap.leg.br/api/materia/materialegislativa/...\n",
      "Processing page 1 of https://sapl.serradonavio.ap.leg.br/api/materia/materialegislativa/...\n",
      "Processing https://sapl.missaovelha.ce.leg.br/api/materia/materialegislativa/...\n",
      "Processing page 1 of https://sapl.missaovelha.ce.leg.br/api/materia/materialegislativa/...\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Check if progress file exists\n",
    "progress_file = \"progress.txt\"\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, \"r\") as file:\n",
    "        progress_data = file.readlines()\n",
    "    last_processed_index = int(progress_data[0].strip())  # Read the index of the last processed API URL\n",
    "    last_processed_page = int(progress_data[1].strip())  # Read the last processed page\n",
    "    last_processed_next_url = progress_data[2].strip()  # Read the last processed next URL\n",
    "else:\n",
    "    last_processed_index = 0  # If the progress file doesn't exist, start from the beginning\n",
    "    last_processed_page = 1  # Start from the first page\n",
    "    last_processed_next_url = api_urls[0]\n",
    "\n",
    "# Loop through each API URL starting from the last processed index\n",
    "for i in range(last_processed_index, len(api_urls)):\n",
    "    api_url = api_urls[i]\n",
    "    print(f\"Processing {api_url}...\")  # Print current API URL being processed\n",
    "    city_name = api_url.split(\".\")[1]  # Extract city name from the URL\n",
    "    if last_processed_next_url == api_url:\n",
    "        city_data = fetch_all_data(api_url, start_page=last_processed_page)  # Fetch data from the API URL starting from the last processed page\n",
    "    else:\n",
    "        city_data = fetch_all_data(last_processed_next_url, start_page=last_processed_page)  # Fetch data from the API URL\n",
    "    if city_data:\n",
    "        city_df = pd.DataFrame(city_data)\n",
    "        city_df[\"City\"] = city_name\n",
    "        city_df[\"API_URL\"] = api_url\n",
    "        city_df.to_csv(f\"SAPL-Cidades_2025/{city_name}_data.csv\", index=False)\n",
    "        final_df = pd.concat([final_df, city_df], ignore_index=True)\n",
    "    # Update last processed next URL and page for progress\n",
    "    if i < len(api_urls) - 1:  # If not the last URL in the list\n",
    "        last_processed_next_url = api_urls[i + 1]  # Update last processed next URL\n",
    "        last_processed_page = 1  # Reset page number for the next URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c78aa201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame\n",
    "# List to store DataFrames from each CSV file\n",
    "allcities = []\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir('SAPL-Cidades_2025'):\n",
    "    if file_name.endswith(\".csv\"):  # Check if the file is a CSV file\n",
    "        file_path = os.path.join('SAPL-Cidades_2025', file_name)  # Get the full file path\n",
    "        onecity = pd.read_csv(file_path, dtype=str)  # Read the CSV file into a DataFrame\n",
    "        allcities.append(onecity)  # Append the DataFrame to the list\n",
    "\n",
    "# Concatenate all DataFrames into a single one\n",
    "merged_df = pd.concat(allcities, ignore_index=True)\n",
    "merged_df.to_csv(\"ProjCidades2025.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497dadf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
