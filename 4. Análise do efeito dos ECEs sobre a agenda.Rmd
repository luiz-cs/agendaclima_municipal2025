---
title: "4. Análise do efeito dos ECEs sobre a agenda"
output: html_notebook
---

# 4. Análise do efeito dos ECEs sobre a agenda

Agora que já identificamos quais os tópicos dos documentos, vamos fazer uma análise da produção em relação ao total da produção.\
\
Na primeira, faremos a análise com toda a produção que menciona os termos procurados. Na segunda, vamos nos restringir à produção em projetos de lei

## Pacotes e bases de dado

```{r}
#install.packages("patchwork")
#install.packages("plm")
#install.packages("zoo")
#install.packages("collapse")
#install.packages("performance")
```

```{r, echo = FALSE, warning=FALSE}
library(dplyr)
library(here)
library(tidyr)
library(data.table)
library(stringr)
library(ggplot2)
library(lubridate)
library(patchwork)
library(scales)
library(plm)
library(stargazer)
library(zoo)
library(collapse)
library(performance)
```

```{r, echo = FALSE}
df_ols <- readRDS(here("Dados/df_ols.RDS")) 

df_ols[, Dia := as.integer(Dia)]
```

## 4.1 Observando as características da série

### 4.1.1 Distribuição da agenda climática

```{r}
#Heatmap
#Limpar datas:

datas <- df_ols %>% 
  group_by(Data) %>%
  summarise(n = n()) %>% 
  filter(n == max(n))
  
df_heatmap <- df_ols %>% 
  filter(Data %in% datas$Data) %>% 
  mutate(Cod_Reg = str_sub(Cod_IBGE_Mun, 1, 1),
         Regiao = case_when(
           Cod_Reg == 1 ~ "Norte",
           Cod_Reg == 2 ~ "Nordeste",
           Cod_Reg == 3 ~ "Sudeste",
           Cod_Reg == 4 ~ "Sul",
           Cod_Reg == 5 ~ "Centro-Oeste"),
         log_agenda_clima = log1p(agenda_clima))

df_desastres <- df_heatmap %>% 
  filter(Desastre == 1)

regioes <- df_heatmap %>% 
  select(Cod_Reg, Regiao) %>% 
  unique()

# Geração + salvamento dos plots
heatmaps1 <- lapply(c(1:5), function(reg) {
  dados_reg <- df_heatmap %>% filter(Cod_Reg == reg)
  dados_desastres <- dados_reg %>% filter(Desastre == 1)
  Regiao <- regioes %>% 
    filter(Cod_Reg == reg) %>% 
    select(Regiao)

  p <- ggplot(dados_reg, aes(x = Data, y = Cod_IBGE_Mun)) +
    geom_tile(aes(fill = log_agenda_clima)) +
    geom_tile(data = dados_desastres, aes(fill = NULL), color = "red") +
    scale_fill_gradient(low = "lightgreen", high = "darkgreen", na.value = "white") +
    theme_classic() +
    theme(
      legend.position = "none",
      axis.title.x = element_blank(),
      axis.title.y = element_blank()
    ) +
    ggtitle(paste("Percentual de Agenda Climática: Região ", Regiao))

  # Salvar
  ggsave(
    filename = here(paste0("Figuras/4.1.", reg, "heatmap_regiao_", Regiao, ".png")),
    plot = p,
    width = 10, height = 4 + n_distinct(dados_reg$Cod_IBGE_Mun) / 20)

  return(p)  # opcional, se quiser reusar depois
})

rm(heatmaps1, datas, df_heatmap, df_desastres, regioes)
```

## 4.1.2 Padronizando as variáveis explicativas

```{r}

#Transformando variável "Obitos" em dummy
df_ols[, "d2_obitos" := fifelse(Obitos >0, 1, 0)]

#Padronizando variáveis contínuas
varpadronizar <- c(
  "Receitas_pCap", "n_OSCs",
  "AB2_Segurança_Alimentar_Seca", "AB2_Segurança_Alimentar_Chuva",
  "AB2_Desastres_Hidrológicos_Inundações_enxurradas_e_alagamentos",
  "AB2_Desastres_Hidrológicos_Deslizamento_de_terra"
)

# Loop eficiente para criar colunas _pd e dp_
for (var in varpadronizar) {
  desvio <- sd(df_ols[[var]], na.rm = TRUE)
  media  <- mean(df_ols[[var]], na.rm = TRUE)
  
  df_ols[[paste0("dp_", var)]] <- desvio
  df_ols[[paste0(var, "_pd")]] <- (df_ols[[var]] - media) / desvio
}

rm(varpadronizar, media, var, desvio)
```

```{r}
#Renomeando variáveis
df_ols <- df_ols %>% 
  rename("y_agenda_clima" = "agenda_clima", "y2_agenda_estrita" = "agenda_estrita",
         "d1_Desastre" = "Desastre",
         "w_dias_eleicao" = "dias_proxima_eleicao", 
         "x1_Receitas_pCap_pd" = "Receitas_pCap_pd", 
         "x2_n_OSCs_pd" = "n_OSCs_pd",
    "z1_Seca_pd" = "AB2_Segurança_Alimentar_Seca_pd",
         "z2_Chuva_pd" = "AB2_Segurança_Alimentar_Chuva_pd",
         "z3_Inundacoes_pd" = "AB2_Desastres_Hidrológicos_Inundações_enxurradas_e_alagamentos_pd",
         "z4_Deslizamento_pd" = "AB2_Desastres_Hidrológicos_Deslizamento_de_terra_pd")
```

## 4.2 Testando as condições da variável destino para análise de séries temporais

### 4.2.1 Analisando a variável destino

```{r}
#Teste de estacionariedade
# Gráfico ACF
png(here("Figuras/4.2.1Correlograma1.png"), width = 1200, height = 400)
psacf(df_ols, y_agenda_clima ~ Cod_IBGE_Mun, ~ Dia,
      type = "correlation",
      main = "Correlograma ACF da agenda_clima",
      family = "serif")
dev.off()
```

```{r}
# Gráfico PACF
png(here("Figuras/4.2.2Correlograma_Parcial.png"), width = 1200, height = 400)
psacf(df_ols, y_agenda_clima ~ Cod_IBGE_Mun, ~ Dia,
      type = "partial",
      main = "Correlograma PACF da agenda_clima",
      family = "serif")
dev.off()

```

### 4.2.3 Checando por autocorrelação nas variáveis

```{r, warning=FALSE}
#Checando por autocorrelações na matriz das variáveis: 

cor_matrix <- cor(df_ols[, c("y_agenda_clima", "y2_agenda_estrita", 
                             "d1_Desastre", "d2_obitos",
                             "w_dias_eleicao", "x1_Receitas_pCap_pd", "x2_n_OSCs_pd",
                             "z1_Seca_pd", "z2_Chuva_pd","z3_Inundacoes_pd","z4_Deslizamento_pd")],
                  use = "complete.obs") %>% 
  data.table() %>% 
  mutate(Coluna = c("y_agenda_clima", "y2_agenda_estrita",
                    "d1_Desastre", "d2_obitos",
                             "w_dias_eleicao","x1_Receitas_pCap_pd", "x2_n_OSCs_pd",
                    "z1_Seca_pd", "z2_Chuva_pd","z3_Inundacoes_pd","z4_Deslizamento_pd")) 
correlacao1 <- cor_matrix %>% 
  pivot_longer(cols = 1:(length(cor_matrix)-1), names_to = "Linha", values_to = "Valores") %>% 
  ggplot(aes(x= Linha, y = Coluna, fill = Valores)) + 
  geom_tile()+
    labs(
    title = "Matriz de correlação das variáveis",
    x = "Variáveis 2",
    y = "Variáveis 1",
    caption = "Elaborado pelo autor") +
  theme_classic() +
    theme(legend.position = "bottom",
        text = element_text(family = "Times New Roman"),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.text = element_text(size = 10),
        strip.text = element_text(size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1))
  

correlacao1


ggsave(here("Figuras/4.2.3Matriz_Correlação.png"), correlacao1, width = 10, height = 10)
rm(cor_matrix, correlacao1)
```

Como é possível observar, a maioria das variáveis não tem um nível de correlação muito elevado, exceto as da mesma categoria do Índice de Risco Climático.

## 4.2.4 Checando balanceamento

```{r}
#Criando coluna separado dos desvios-padrão originais:

desvios <- df_ols %>% 
  select(starts_with("dp")) %>%
  unique() %>% 
  pivot_longer(cols = starts_with("dp"), names_prefix = "dp_", names_to = "Variável", values_to = "Desvio-Padrão Original")

#Checando missing values:

df_ols <- df_ols %>% 
  select(Cod_IBGE_Mun, Dia, Data, Mes, Ano,
         y_agenda_clima, y2_agenda_estrita,
         d1_Desastre, d2_obitos,
         w_dias_eleicao, x1_Receitas_pCap_pd, x2_n_OSCs_pd,
         z1_Seca_pd, z2_Chuva_pd,z3_Inundacoes_pd,z4_Deslizamento_pd)

colSums(is.na(df_ols))
```

Faltam 5833 observações para a variável de Receita. Vamos verificar quais são os municípios faltantes:

```{r}
Receitas_NA <- df_ols %>% 
  mutate(n = ifelse(is.na(x1_Receitas_pCap_pd), 1, 0)) %>% 
    group_by(Cod_IBGE_Mun, Ano) %>% 
  summarise(missing = sum(n)) %>% 
  filter(missing > 0)

nome_munics <- readRDS(here("Dados/df_agendaclima_municipal.RDS")) %>% 
  select(Cod_IBGE_Mun, Nome_Municipio, UF) %>% 
  unique()

Receitas_NA <- merge(Receitas_NA, nome_munics, by = "Cod_IBGE_Mun", all.x = T)

```

Como é possível observar, temos 14 cidades com Missing Values para a receita. Como o número de observações faltantes corresponde a menos de 0,5% das observações, e o foco da análise está na variação agregada, optei opor imputar os dados faltantes.

```{r}

# Ordenando os dados
setkey(df_ols, Cod_IBGE_Mun, Dia)

# Realizando a imputação>
df_ols[, x1_Receitas_pdi := na.approx( #Interpolação
  x1_Receitas_pCap_pd,
  x = Dia,
  na.rm = FALSE,
  rule = 2 #Extrapolação
), by = Cod_IBGE_Mun]

colSums(is.na(df_ols))


rm(nome_munics, Receitas_NA)
```

## 4.3 Rodando a regressão com série temporal

### 4.3.1 Análise com todas as variáveis - Modelo Pooling

```{r}
#Preparando base de dados 
df_ols <- df_ols %>% 
  pdata.frame(index = c("Cod_IBGE_Mun", "Dia"))

```

### Modelo 1: Pooling

```{r}

#Primeito modelo: agenda mensurada como percentual com menção a termos climáticos

modelo1 <- plm(y_agenda_clima ~ lag(y_agenda_clima) + 
                 d1_Desastre + lag(d1_Desastre) + 
         d2_obitos + lag(d2_obitos)+
         w_dias_eleicao+ x1_Receitas_pCap_pd+ x2_n_OSCs_pd+
         z1_Seca_pd+ z2_Chuva_pd + z3_Inundacoes_pd + z4_Deslizamento_pd,
         data = df_ols, model = "pooling")

stargazer(modelo1, type = "text", mean.sd = TRUE)
```

### Visualizando os resultados

```{r, warning=FALSE}
# Extrair coeficientes e erros padrão diretamente do objeto plm
coefs <- modelo1$coefficients
variaveis <- names(coefs)
estimativas <- coefs

# Extrair erros padrão da matriz de variâncias-covariâncias
erros_padrao <- sqrt(diag(modelo1$vcov))

# Calcular p-valores (opcional, com base na distribuição normal)
t_values <- estimativas / erros_padrao
pvalores <- 1.96 * (1 - pnorm(abs(t_values)))

# Definir nível de significância
nivel_sig <- cut(pvalores, breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf), 
                 labels = c("0,1%***", "1%**", "5%*", "10%.", " "))

# Construir data.frame
m1_plot <- data.frame(
  Variavel = factor(variaveis, levels = variaveis), # manter a ordem
  Estimativa = estimativas,
  Erro_Padrao = erros_padrao,
  IC_inferior = estimativas - 1.96 * erros_padrao,
  IC_superior = estimativas + 1.96 * erros_padrao,
  Significancia = nivel_sig
)

# Remover intercepto para focar nos preditores
#m1_plot <- subset(m1_plot, Variavel != "(Intercept)")

# Plot com ggplot2
M1plot <- ggplot(m1_plot, aes(x = Variavel, y = Estimativa, color = Significancia)) +
  geom_pointrange(aes(ymin = IC_inferior, ymax = IC_superior)) +
  geom_hline(yintercept = 0, color = "black") +
  scale_color_manual(values = c("0,1%***"="#6E0000", "1%**"="#CB8400", "5%*"="#FFD028", "10%."="#F7FF00", " "="gray"),
                     name = "Nível de\nSignificância") +
  coord_flip() +
  theme_bw() +
    theme(legend.position = "bottom",
        text = element_text(family = "Times New Roman"),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.text = element_text(size = 10),
        strip.text = element_text(size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(
    title = "Coeficientes Modelo 1 (Pooled)",
    x = "Variável",
    y = "Estimativa",
    caption = "Elaborado pelo autor")+
    scale_y_continuous(labels = label_percent(accuracy = 1))

M1plot

ggsave(here("Figuras/4.3.1.1Modelo1.png"), M1plot, width= 10, height = 5)

rm(variaveis, estimativas, erros_padrao, t_values, pvalores, nivel_sig, m1_plot)
```

```{r, warning=FALSE}
M1plot_z <- M1plot+ scale_y_continuous(labels = label_percent(accuracy = 0.001), limits = c(-0.0025, 0.005))

M1plot_z

ggsave(here("Figuras/4.3.1.2Modelo1_zoom.png"), M1plot_z, width= 10, height = 5)

rm(M1plot, M1plot_z)
```

```{r}
summary(df_ols$y_agenda_clima)
```

### Testes dos pressupostos da regressão linear

```{r}
checagem <- check_model(modelo1)
```

### Teste de autocorrelação nas unidades (Pesaran)

```{r}

TestePesaran1 <- pcdtest(modelo1, test = "cd")
print(TestePesaran1)

saveRDS(TestePesaran1, here("Dados/PesaranM1.RDS"))

rm(TestePesaran1)
```

### Teste de autocorrelação serial de Durbin-Watson

```{r}
#Teste de Durbin-Watson:
TesteDW1 <- pdwtest(modelo1)

print(TesteDW1)

saveRDS(TesteDW1, here("Dados/M1DurbinWatson.RDS"))

rm(TesteDW1)
```

### \# Teste de Breusch-Godfrey para autocorrelação serial com "pbgtest"

```{r}
{r}
# Teste de Breusch-Godfrey com "pbgtest"


TesteBG <- pbgtest(agenda_clima ~ lag(agenda_clima), data = df_ols)


TesteBG

#rm(TesteBG)
```

### Efeito de longo prazo da ocorrência de desastres

```{r}
# Long-term effect
lte_total <- (coefs["d1_Desastre"] + coefs["lag(d1_Desastre)"]) / (1 - coefs["lag(y_agenda_clima)"])


print(paste("Efeito de Longo Prazo do Desastre:", round(lte_total, 3), " ou ", round(lte_total/mean(df_ols$y_agenda_clima), 3), "vezes a média de proposituras")) 

rm(lte_total, coefs)
```

### 4.3.2 - Modelo "Within"

```{r}
#Segunda modelo: agenda mensurada como percentual com menção a termos climáticos, dentro dos municípios

modelo2 <- plm(agenda_clima ~ lag(agenda_clima)+
                 Desastre + lag(Desastre) + Obitos + lag(Obitos) + 
                 dias_proxima_eleicao +  
                 Receitas_pCap + n_OSCs +
                 AB2_Recursos_Hídricos_Seca + 
                 AB2_Segurança_Alimentar_Seca + AB2_Segurança_Alimentar_Chuva +                AB2_Desastres_Hidrológicos_Inundações_enxurradas_e_alagamentos + AB2_Desastres_Hidrológicos_Deslizamento_de_terra, 
               data = df_ols, model = "within")

stargazer(modelo2, type = "text")
```

### Visualizando os resultados

```{r}
# Extrair coeficientes e erros padrão diretamente do objeto plm
coefs <- modelo2$coefficients
variaveis <- names(coefs)
estimativas <- coefs

# Extrair erros padrão da matriz de variâncias-covariâncias
erros_padrao <- sqrt(diag(modelo2$vcov))

# Calcular p-valores (opcional, com base na distribuição normal)
t_values <- estimativas / erros_padrao
pvalores <- 1.96 * (1 - pnorm(abs(t_values)))

# Definir nível de significância
nivel_sig <- cut(pvalores, breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf), 
                 labels = c("0,1%***", "1%**", "5%*", "10%.", " "))

# Construir data.frame
m2_plot <- data.frame(
  Variavel = factor(variaveis, levels = variaveis), # manter a ordem
  Estimativa = estimativas,
  Erro_Padrao = erros_padrao,
  IC_inferior = estimativas - 1.96 * erros_padrao,
  IC_superior = estimativas + 1.96 * erros_padrao,
  Significancia = nivel_sig
)

# Remover intercepto para focar nos preditores
#m1_plot <- subset(m1_plot, Variavel != "(Intercept)")

# Plot com ggplot2
M2plot <- ggplot(m2_plot, aes(x = Variavel, y = Estimativa, color = Significancia)) +
  geom_pointrange(aes(ymin = IC_inferior, ymax = IC_superior)) +
  geom_hline(yintercept = 0, color = "black") +
  scale_color_manual(values = c("0,1%***"="#6E0000", "1%**"="#CB8400", "5%*"="#FFD028", "10%."="#F7FF00", " "="gray"),
                     name = "Nível de\nSignificância") +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "bottom")+
  labs(
    title = "Modelo 1 - Coeficientes (95%)",
    x = "Variável",
    y = "Estimativa"
  )

M2plot

rm(coefs, variaveis, estimativas, erros_padrao, t_values, pvalores, nivel_sig, M2plot, m2_plot)
```

### Teste de autocorrelação nas unidades (Pesaran)

```{r}

TestePesaran2 <- pcdtest(modelo2, test = "cd")
print(TestePesaran2)

rm(TestePesaran2)
```

### Teste de autocorrelação serial de Durbin-Watson

```{r}
#Teste de Durbin-Watson:
TesteDW2 <- pdwtest(modelo2)

print(TesteDW2)

rm(TesteDW2, modelo2)
```

## 4.4 Teste com agenda climática estrita

Agora, vamos repetir a análise, utilizando apenas os projetos identificados na segunda modelagem de tópicos como condizentes com a agenda em adaptação climática.

```{r}

df_stm <- readRDS(here("Dados/df_STM_PL.RDS")) %>% 
  filter(!topico_principalPL %in% c("Tópico PL1", "Tópico PL5", "Tópico PL6", "Tópico PL7", "Tópico PL8")) %>% #Removendo tópicos que não dizem respeito à política climática
  group_by(Data, Cod_IBGE_Mun) %>% 
  summarise(N_PLsClima = n( ),
            Total_PLs = sum(total_tipo_proj_dia)) %>% 
  mutate(PL_agendaclima = N_PLsClima/Total_PLs)

#Juntando à outra base e completando as informações que faltam
df_stm <- merge(df_ols, df_stm, by = c("Data", "Cod_IBGE_Mun"), all.x = T) %>% 
  mutate(N_PLsClima = ifelse(is.na(N_PLsClima), 0, N_PLsClima),
         PL_agendaclima = ifelse(is.na(PL_agendaclima), 0, PL_agendaclima))%>% 
  mutate(Ano_Mes = paste0(Ano, "-", Mes)) %>% 
  data.table()

#Adicionando meses até próxima eleição
df_stm[, meses_proxima_eleicao := interval(Data, proxima_eleicao) %/% months(1)]


df_stm <- df_stm %>% 
  group_by(Ano_Mes, Cod_IBGE_Mun, Ano, Mes, Receitas_pCap, proxima_eleicao, AB2_Recursos_Hídricos_Seca, AB2_Segurança_Alimentar_Seca, AB2_Segurança_Alimentar_Chuva,       AB2_Desastres_Hidrológicos_Inundações_enxurradas_e_alagamentos, AB2_Desastres_Hidrológicos_Deslizamento_de_terra) %>% 
  summarise(N_PLsClima = sum(N_PLsClima),
            Total_PLs = sum(Total_PLs),
            Desastre = sum(Desastre),
            Obitos = sum(Obitos),
            n_OSCs = max(n_OSCs),
            eleicao = sum(eleicao),
            meses_proxima_eleicao = max(meses_proxima_eleicao),
            total_proj_dia = sum(total_proj_dia)) %>% 
  filter(meses_proxima_eleicao != 0)


```

```{r}
#Heatmap 2 
#Limpar datas:

  
df_heatmap <- df_stm %>% 
  mutate(Cod_Reg = str_sub(Cod_IBGE_Mun, 1, 1),
         Regiao = case_when(
           Cod_Reg == 1 ~ "Norte",
           Cod_Reg == 2 ~ "Nordeste",
           Cod_Reg == 3 ~ "Sudeste",
           Cod_Reg == 4 ~ "Sul",
           Cod_Reg == 5 ~ "Centro-Oeste"))%>% 
  arrange(desc(Cod_IBGE_Mun))


df_desastres <- df_heatmap %>% 
  filter(Desastre == 1) %>% 
  arrange(desc(Cod_IBGE_Mun))

regioes <- df_heatmap %>% 
  ungroup() %>% 
  select(Cod_Reg, Regiao) %>% 
  unique()

# Geração + salvamento dos plots
heatmaps2 <- lapply(c(1:5), function(reg) {
  dados_reg <- df_heatmap %>% filter(Cod_Reg == reg)
  dados_desastres <- dados_reg %>% filter(Desastre == 1)
  Regiao <- regioes %>% 
    filter(Cod_Reg == reg) %>% 
    select(Regiao)

  p <- ggplot(dados_reg, aes(x = Ano_Mes, y = Cod_IBGE_Mun)) +
    geom_tile(aes(fill = N_PLsClima)) +
    geom_tile(data = dados_desastres, aes(fill = N_PLsClima), color = "red", size = 0.25) +
    scale_fill_gradient(low = "lightgreen", high = "darkgreen", na.value = "white") +
    theme_classic() +
    theme(
      legend.position = "bottom",
      axis.title.x = element_blank(),
      axis.title.y = element_blank()
    ) +
    scale_x_discrete(breaks = function(x) x[seq(1, length(x), by = 6)])+
    ggtitle(paste("Produção de Políticas Climáticas: Região ", Regiao))

  # Salvar
  ggsave(
    filename = here(paste0("Figuras/4.4.", reg, "PLheatmap_regiao_", Regiao, ".png")),
    plot = p,
    width = 10, height = 4 + n_distinct(dados_reg$Cod_IBGE_Mun) / 20)

  return(p) 
})

rm(heatmaps2, datas, df_heatmap, df_desastres, regioes)
```

### 4.4.3 Testes de sazonalidade

```{r}

sazonalidade2 <- df_stm %>% 
  group_by(Cod_IBGE_Mun, Mes) %>% 
  summarise(N_PLsClima = sum(N_PLsClima)) %>% 
  group_by(Mes) %>% 
  summarise(N_PLsClima = mean(N_PLsClima)) %>% 
  ggplot(aes(x = Mes, y = N_PLsClima)) +
  geom_col(fill = "darkgreen")+
  theme_bw()+
  labs(title = "Média de produção legislativa relacionada ao clima, por mês",
       x= "Mês", 
       y = "Percentual de produção legislativa")+
  scale_x_continuous(n.breaks = 12)

sazonalidade2

rm(sazonalidade2)
```

### 4.4.4 Checando por autocorrelação nas variáveis

```{r}
#Checando por autocorrelações na matriz das variáveis: 

cor_matrix <- cor(df_stm[, c("N_PLsClima", "Desastre", "Obitos",
                             "eleicao", "Receitas_pCap", "n_OSCs",
                             "AB2_Recursos_Hídricos_Seca", 
                             "AB2_Segurança_Alimentar_Seca", "AB2_Segurança_Alimentar_Chuva",
                             "AB2_Desastres_Hidrológicos_Inundações_enxurradas_e_alagamentos",
                             "AB2_Desastres_Hidrológicos_Deslizamento_de_terra")],
                  use = "complete.obs") %>% 
  data.table() %>% 
  mutate(Coluna = c("N_PLsClima", "Desastre", "Obitos",
                             "eleicao", "Receitas_pCap", "n_OSCs",
                             "AB2_Recursos_Hídricos_Seca", 
                             "AB2_Segurança_Alimentar_Seca", "AB2_Segurança_Alimentar_Chuva",
                             "AB2_Desastres_Hidrológicos_Inundações_enxurradas_e_alagamentos",
                             "AB2_Desastres_Hidrológicos_Deslizamento_de_terra")) 
correlacao1 <- cor_matrix %>% 
  pivot_longer(cols = 1:(length(cor_matrix)-1), names_to = "Linha", values_to = "Valores") %>% 
  ggplot(aes(x= Linha, y = Coluna, fill = Valores)) + 
  geom_tile()

correlacao1

rm(cor_matrix, correlacao1)
```

Como é possível observar, a maioria das variáveis não tem um nível de correlação muito elevado, exceto as da mesma categoria do Índice de Risco Climático.

```{r}
#Checando missing values:

colSums(is.na(df_stm))
```

### 4.4.5 Teste de Autocorrelação Serial

```{r}
# Teste de Breusch-Godfrey com "pbgtest"

df_stm <- pdata.frame(df_stm, index = c("Cod_IBGE_Mun", "Ano_Mes"))

TesteBG2 <- pbgtest(N_PLsClima ~ lag(N_PLsClima), data = df_stm)


TesteBG2

rm(TesteBG2)
```

### 4.4.6 Modelo PL1: Pooled

```{r}
check <- df_stm %>% 
  group_by(Ano_Mes) %>% 
  summarise( n = n())
#Primeito modelo: agenda mensurada como percentual com menção a termos climáticos

mPL1 <- plm(N_PLsClima ~ lag(N_PLsClima) + 
                 Desastre + lag(Desastre) + Obitos + lag(Obitos) + 
                 eleicao +  lag(eleicao) + 
                 Receitas_pCap + n_OSCs +
                 AB2_Recursos_Hídricos_Seca + 
                 AB2_Segurança_Alimentar_Seca + AB2_Segurança_Alimentar_Chuva +
                 AB2_Desastres_Hidrológicos_Inundações_enxurradas_e_alagamentos +
                 AB2_Desastres_Hidrológicos_Deslizamento_de_terra, 
               data = df_stm, model = "pooling", pooling = T)

stargazer(mPL1, type = "text")
```

### Visualizando os resultados

```{r}
# Extrair coeficientes e erros padrão diretamente do objeto plm
coefs <- mPL1$coefficients
variaveis <- names(coefs)
estimativas <- coefs

# Extrair erros padrão da matriz de variâncias-covariâncias
erros_padrao <- sqrt(diag(mPL1$vcov))

# Calcular p-valores (opcional, com base na distribuição normal)
t_values <- estimativas / erros_padrao
pvalores <- 1.96 * (1 - pnorm(abs(t_values)))

# Definir nível de significância
nivel_sig <- cut(pvalores, breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf), 
                 labels = c("0,1%***", "1%**", "5%*", "10%.", " "))

# Construir data.frame
m2_plot <- data.frame(
  Variavel = factor(variaveis, levels = variaveis), # manter a ordem
  Estimativa = estimativas,
  Erro_Padrao = erros_padrao,
  IC_inferior = estimativas - 1.96 * erros_padrao,
  IC_superior = estimativas + 1.96 * erros_padrao,
  Significancia = nivel_sig
)

# Remover intercepto para focar nos preditores
#m1_plot <- subset(m1_plot, Variavel != "(Intercept)")

# Plot com ggplot2
M2plot <- ggplot(m2_plot, aes(x = Variavel, y = Estimativa, color = Significancia)) +
  geom_pointrange(aes(ymin = IC_inferior, ymax = IC_superior)) +
  geom_hline(yintercept = 0, color = "black") +
  scale_color_manual(values = c("0,1%***"="#6E0000", "1%**"="#CB8400", "5%*"="#FFD028", "10%."="#F7FF00", " "="gray"),
                     name = "Nível de\nSignificância") +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "bottom")+
  labs(
    title = "Coeficientes Modelo PL 1 (Pooled)",
    x = "Variável",
    y = "Estimativa"
  )

M2plot

rm(coefs, variaveis, estimativas, erros_padrao, t_values, pvalores, nivel_sig)
```

```{r}
TestePesaran2 <- pcdtest(mPL1, test = "cd")
print(TestePesaran2)

rm(TestePesaran2)
```

```{r}
#Teste de Durbin-Watson:
TesteDW2 <- pdwtest(mPL1)

print(TesteDW2)

rm(TesteDW2)
```

### 4.4.7 Modelo 2: Within

```{r}

#Primeito modelo: agenda mensurada como percentual com menção a termos climáticos

mPL2 <- plm(N_PLsClima ~ lag(N_PLsClima) + 
                 Desastre + lag(Desastre) + Obitos + lag(Obitos) + 
                 eleicao +  lag(eleicao) + 
                 Receitas_pCap + n_OSCs +
                 AB2_Recursos_Hídricos_Seca + 
                 AB2_Segurança_Alimentar_Seca + AB2_Segurança_Alimentar_Chuva +
                 AB2_Desastres_Hidrológicos_Inundações_enxurradas_e_alagamentos +
                 AB2_Desastres_Hidrológicos_Deslizamento_de_terra, 
               data = df_stm, model = "within", pooling = T)


stargazer(mPL2, type = "text")
```

### Visualizando os resultados

```{r}
# Extrair coeficientes e erros padrão diretamente do objeto plm
coefs <- mPL2$coefficients
variaveis <- names(coefs)
estimativas <- coefs

# Extrair erros padrão da matriz de variâncias-covariâncias
erros_padrao <- sqrt(diag(mPL2$vcov))

# Calcular p-valores (opcional, com base na distribuição normal)
t_values <- estimativas / erros_padrao
pvalores <- 1.96 * (1 - pnorm(abs(t_values)))

# Definir nível de significância
nivel_sig <- cut(pvalores, breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf), 
                 labels = c("0,1%***", "1%**", "5%*", "10%.", " "))

# Construir data.frame
m2_plot <- data.frame(
  Variavel = factor(variaveis, levels = variaveis), # manter a ordem
  Estimativa = estimativas,
  Erro_Padrao = erros_padrao,
  IC_inferior = estimativas - 1.96 * erros_padrao,
  IC_superior = estimativas + 1.96 * erros_padrao,
  Significancia = nivel_sig
)

# Remover intercepto para focar nos preditores
#m1_plot <- subset(m1_plot, Variavel != "(Intercept)")

# Plot com ggplot2
M2plot <- ggplot(m2_plot, aes(x = Variavel, y = Estimativa, color = Significancia)) +
  geom_pointrange(aes(ymin = IC_inferior, ymax = IC_superior)) +
  geom_hline(yintercept = 0, color = "black") +
  scale_color_manual(values = c("0,1%***"="#6E0000", "1%**"="#CB8400", "5%*"="#FFD028", "10%."="#F7FF00", " "="gray"),
                     name = "Nível de\nSignificância") +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "bottom")+
  labs(
    title = "Modelo 1 - Coeficientes (95%)",
    x = "Variável",
    y = "Estimativa"
  )

M2plot

rm(coefs, variaveis, estimativas, erros_padrao, t_values, pvalores, nivel_sig)
```

```{r}
TestePesaran2 <- pcdtest(mPL2, test = "cd")
print(TestePesaran2)

rm(TestePesaran2)
```

```{r}
#Teste de Durbin-Watson:
TesteDW2 <- pdwtest(mPL2)

print(TesteDW2)

rm(TesteDW2)
```
